---
title: Whose Fault When Machines Err? / 程序出错，谁的责任？
category: 考察
date: 2025-06-03 11:00:28
tags:
  - 旧作
---
This is the translation of my article originally in Chinese published online on 19 Oct 2017. The original post on Jianshu has been 'locked' by the platform and is no more publically accessible. Here I make a new post, because I believe the discussion of ethics in AI is more important than ever.

To save time, I used ChatGPT (4o date: 3 Jul 2025) to produce a first draft of the translation, and then modified the draft as needed. 

The original article in Chinese was attached in the end of this post.


chatgpt failed to:
mention 李世乭

IBM quote

---

> *Starting __tabula rasa__, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.* —Mastering the game of Go without human knowledge [^1]

# I. Machines and Program
'Program' is typically associated with computers and programmers. Together with other computer science jargons, such as artificial intelligence, machine learning, pattern recognition, neural networks. they have been brought into the public domain thanks to recent technological achievements. For instance, AlphaGo stunned the world by defeating the world Go champion, Lee Sedol, and the new version, AlphaGo Zero, decisively beated AlphaGo with a 100:0 record. Autonomous vehicles, once running only in laboratories, now frequently makes headlines as deployed on real-world roads. 

However, the concept of 'program' can be extended beyond machines—to people, organisations, nations, even the entire (human) world. For example, if someone discovers extraterrestrial life, they are obligated by the United Nations' [Space Law](http://www.unoosa.org/oosa/en/ourwork/spacelaw/index.html) to notify the Secretary-General. International laws, treaties, and agreements such as [Nuclear Non-Proliferation Treaty](https://www.un.org/disarmament/wmd/nuclear/npt/)—irritating the U.S. President Trump—and the [Paris Agreement](http://unfccc.int/paris_agreement/items/9485.php)—from which Trump orderd to exit, irritating the rest of the world—are other examples of 'programs' for regulating human behaviours. At a smaller scale, a country may have explicit laws for protecting children, a political party or a company may have a charter, and individual people may use a fixed routine for their weddings, funerals, or their morning Tai Chi exercises.

Indeed, any sequence of actions or operations that must be executed in a specific way can be considered as a 'program'. The (nearly) same concept may be called differently given different contexts. 'Code' is probabily a word with greater compatibility with both machines and humans, but 'program' is used here for less ambiguity.

It's actually meaningful to consider all examples above as 'programs', because they all aim to achieve specific results through a fixed set of actions. An alarm clock set for 7:00 will ring at 7:00—mechanical or digital. The U.S. holds a presidential election every four years. Programs usually work—but they can fail. The alarm might run out of power. The president might be assassinated.

_'What happens when things go wrong?'_ is the most immediate question for programs—operating a mechnical/digital or legal systems, and our answers reveal differences of such systems. A dead battery requires recharging. A sudden presidential death invokes automatic succession. But note: recharging must be done externally—the powerless machine can’t act. A government without a president can appoint one—it’s already part of the legal process.

'Autonomy' is the crucial difference between humans and machines. Machine programs are mandatory, logical, and grounded in physical laws. Their strength is efficiency and reliability—within expected error tolerance. But beyond the tolerance, they crash. Legal systems are also compulsory but strive to minimise coercion, because they deal with the most peculiar human trait—freedom. Typically, they regulate human behaviour by encouraging 'good' conduct and penalising 'bad' to promote collective benefit.

At this point, readers may argue machines can evolve toward autonomy—given enough complexity and advancement. Self-driving once struggled with real-world unpredictability—challenges gradually overcome by ever-evolving technology. One day, computers may not only complete tasks but also analyse massive data sets to assist human decision-making, and achieve other amazing things. I’ll address this argument in the following sections.

---

## II. Learning and Understanding

Modern AI owes its strength to the rapid development of "machine learning." This essay won’t delve into technicalities—suffice it to say that today’s programs can solve problems without explicit instructions. Recall AlphaGo Zero: it didn’t study human game records. Given only the basic rules—how to play, capture, and win—it played itself for three days and nights, mastered Go, and surpassed all humans and its predecessor.

It received from its creators only one thing: the _ability to learn_. Though lacking intelligence, it used its vast computation and memory, its tireless persistence, to develop techniques—a strategy for playing and winning. It thus differs from basic programs. This was a program birthed _from_ another program called "learning." Machine learning thrives precisely because this learning algorithm can be applied across vastly different fields—including, incredibly, _art_.

Above is a Rembrandt-style painting created by an AI after learning from the master's works. Dazzling—thrilling or chilling?

But I must say—

_So what?_

I don’t doubt the technology. But like those who ask, “What’s the use of your research?” I ask, what’s the use of machine learning?

If the answer is problem-solving, I have no objection. I got my driver’s license after high school and never drove once. Full self-driving? Perfect. Research shows machine drivers are safer than humans. Automatic elevators were once controversial too; now they’re safer and better-prepared for emergencies.

But my own research aims to deepen understanding of neural signal transmission. It’s rationalist in methodology, supported by empiricism—using mathematical models to deduce theory, then comparing it with experiments. I’m not trying to simulate a neuron or get perfect predictive outputs—though those might happen. If I just wanted results, I’d use off-the-shelf software. Likewise, why spend decades proving mathematical conjectures already tested millions of times? Because the point is not (only) results—it’s deeper _understanding_ of nature.

Back to Go. Machines have proven they can win—far beyond the best humans. No doubt. But what good does that do us? AlphaGo Zero's creators claim it can discover knowledge. So can I, a beginner, learn from it and become a Go master? In reality, many of its moves defy comprehension. To improve, we still need human guidance.

Also, is Go really about winning? A recent case involved a Japanese shogi player suspended for allegedly using a phone to consult an AI mid-game. If all players used AIs, fine. But selectively doing so is unfair—like doping. Acknowledging AI’s usefulness doesn’t mean it's ethically appropriate. There’s a moral gap between completing a task and adhering to laws and norms. Using machine programs to replace legal ones carries ethical risks. In human contests, _fairness before victory_ isn’t empty talk.

Then should we fairly allow everyone to use machines? Perhaps. Chess tournaments now allow human-AI teams. Unlike doping, using computer advice doesn’t harm health. But why do we still run races when anyone on a motorcycle outruns Olympic champions? Machines could make us "faster, higher, stronger" than the Greek gods. So why uphold the Olympic spirit?

Before answering that, let me summarise. While machines outperform humans in specific tasks, they lack ethics and spirit. Some may object—I’ve dodged complexity by dragging in moral and spiritual ambiguity. If those could be clearly defined, surely machines could replace legal codes?

Yet, even if we intuitively _understand_ such concepts, how do we define them? A final, imperfect example: with the same board and pieces, we can play Go, Gomoku, or Reversi. We can teach ourselves or machines any of these. Soon, we lose at all of them. Yet we prefer some, excel at some, hate others—maybe we’re too crushed to play again. We grasp the joy and pain of play. But that machine winning everything?

I originally meant to end here, provoking thought with “I told you—legal codes regulate ambiguity.” But to avoid accusations of evasive mysticism, I’ll pose more concrete questions in the next section.

---

## III. Programs and Responsibility

Let’s return to the problem: unclear goals, legal and moral ambiguity, human emotion—these are hard not because of machines, but _because of us_. Machine programs struggle with them precisely because _we_ don’t understand them. Can legal codes do better?

**_No._**

Legal codes—whether law, certificates, contracts, or ethics, customs, agreements—are never perfect. Even the simplest task, like parents waking a child for school, can fail because humans can choose not to act. But _because_ humans are autonomous, they can also adapt—call someone else, ask for help.

Such programs are flexible not just technically, but because _humans_, their agents, are infinitely adaptable. A tomato-egg dish, following a recipe, won’t go wrong. A machine can make it too. Prefer sweeter? Add sugar—easily done. But maybe today you’re tired of sweets, or want a prettier plate for guests, or in clanging pots invent a new tune that becomes a hit… A machine focused on taste won’t care for beauty or sound. Add those? Try finding an investor for a "singing, automated tomato-egg chef."

But more critically, the key issue isn’t “_what to do when things go wrong_” (technical)—it’s “_who’s responsible_” (ethical). Back to self-driving. Many places allow autonomous cars on roads. But when accidents happen, the _driver_ remains legally liable. Even though self-driving is safer, the one who “hired” the elite AI driver pays the price.

If we compare to elevators, maintenance and liability fall to manufacturers, who must ensure no sabotage—thus full surveillance and monitoring. At that point, who needs the elite driver?

This brings to mind Stanislav Petrov, the Soviet officer who died recently. In 1983, an early-warning system falsely indicated U.S. missiles en route. Petrov, on duty, broke protocol, judged it a false alarm, and _did not_ report. In Cold War conditions, reporting likely meant retaliation—and possibly none of us here today.

In an interview, he recalled: “_The computer data was clear. If I had reported it, no one would have doubted me._” Thankfully, the USSR didn’t entrust humanity’s fate to a machine—whether due to tech limits or power politics is irrelevant. What matters is: when humans are in the loop, _they_ bear responsibility. Legal codes make people most accountable _precisely_ when they break the code.

Another trait: the more people involved in a legal code, the smaller the individual responsibility. “The law doesn’t punish the masses.” Corporations with fewer investors tend toward unlimited liability; those with many lean toward limited. Machine programs don’t share this feature. Their design _reduces_ human involvement—ironically making designers or operators more culpable.

Setting aside system design or power structures, we return to the essential ethical dilemma:

> **_When a program fails, who is responsible?_**

Theoretically simple—like Liu Bang’s three laws: “Killers die, attackers and thieves are punished.” But in practice—how to prove murder? What equals punishment? Many truths remain forever unknown. Legal codes are flawed, exploitable. But the people they regulate—and those who exploit them—are flawed too. Because no one controls them fully, and they can’t control all, legal codes are both error-prone and vital: a compromise of individual will and collective wisdom.

1. A is immoral;
    
2. B is immoral;
    
3. The person in question must choose A or B.
    

---

## Conclusion

1. Of all the challenges machine programs face, technical ones are trivial; ethical ones are not.
    
2. Ethical challenges matter because _we_ don’t fully understand them.
    
3. We can’t hand ethical decisions to machines because when they fail, we don’t know _who_ to blame.
    

Finally, I wanted to quote the Enlightenment, but couldn’t improve on an [earlier piece](http://www.jianshu.com/p/2def48672024):

> _The Enlightenment's praise of reason—should not only guide scientific research but also self-knowledge and social civilisation._

---

>*Starting __tabula rasa__, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.* -- Mastering the game of Go without human knowledge [^1]

[^1]: Silver D, Schrittwieser J, Simonyan K, et al. (2017) [Mastering the game of Go without human knowledge](https://www.nature.com/nature/journal/v550/n7676/pdf/nature24270.pdf). *Nature*, 550: 354–359.

# 一、机器与律法
如今说起「程序」，我们大抵会联想到计算机，程序员。新版本AlphaGo Zero横空出世，更是以100：0的战绩完胜之前击败李世乭的AlphaGo；自动驾驶在前些年还是实验室里的理想，现在国内外自动驾驶汽车上路的新闻不绝于耳。如此种种，顺道把一些理论名词也带入了当代人的常识之中：人工智能，机器学习，模式识别，神经网络……

但程序不仅仅是指上述这些可以用来运作机器的计算机程序，它也可以用来调控人、组织、国家，乃至整个世界。譬如一旦有人发现外星人，联合国有一套规则（[Space Law](http://www.unoosa.org/oosa/en/ourwork/spacelaw/index.html)）指明他有义务通知联合国秘书长。譬如最近让美国总统头疼的《[不扩散核武器条约](https://www.un.org/disarmament/wmd/nuclear/npt/)》以及他让别人头疼退出的《[巴黎协定](http://unfccc.int/paris_agreement/items/9485.php)》等等国际公约。譬如《未成年人保护法》之类的法律。再譬如一个政党或一个公司的章程。甚至可以是个人的婚礼、葬礼，或他每天起床后都要打一套陈氏太极。

凡是一套特定的行动或操作必须以相同的方式执行，都可以叫做程序。其实在英文或港台用语里，两者是用不同的单词的。这个广义的程序就叫做「程序（procedure）」，而较为狭义的计算机程序则是「程式（program）」。另外，法律程序在英文里则用另一个单词「process」。注意在本文中为了避免定义误会导致歧义，笔者使用「律法」这个较为不常用的词来指代与机器程序相对应的，所有暧昧的非机器的程序。在这里强调「暧昧」有两个原因。其一是因为这个定义本身暧昧，其二则是因为它所调整的对象「人」是极其暧昧不清的。

说起来中文之所以把上述所有不同的程序都译作程序，并非毫无道理。因为所有程序的目的都是通过既定的行为操作，以期达成既定的结果。譬如一个设定在七点响铃的闹钟，无论是机械的还是电子的，只要不出问题都会在七点响铃。譬如美国每四年进行一次总统大选，一般情况下也正是如此。然而，上述程序均可能出错。闹钟（或手机）可能没电了，总统可能被暗杀了。

「*出现问题了怎么办？*」是机器程序与律法程序最直接面对的问题，回答这个问题的不同方式也正好反映了两者的不同。没电了要充电，总统在任期内突然死亡则副总统继任——这两个解决方法看似一致，无非是填补上程序缺少的东西。但充电不是一个没有电的机器可以自己完成的事，新总统继任则是一个没有总统的政府可以做到的，或者说这样的考虑早是法律条文，已经成为了程序的一部分。

「自主性」正是人与机器的最大不同，也因此导致两种程序大不相同。机器程序是强制性的、命令性的，利用或具备自然规律的数理逻辑。好处是其目的性强，不容易出错（尤其对于一个具有相当容错率的程序来说），坏处是一旦错误超出预期（处理能力），就会死机。律法程序虽然也是强制性的，但却试图把强制的成分降到最少，因为这里涉及到只有人类才有的最为奇怪的东西——「自由」。一般来说，律法通过鼓励「好的」行为，惩处「坏的」行为，来调整个体的行为，以达到有利于集体的目的[^2]。

[^2]: 当然，上文已经说过，律法程序是暧昧的产物，因此既有试图违反乱纪的个人，亦有采取命令手段剥夺个人自由的组织。这些特例很有探讨价值，但与本文主旨无关，在此不作展开。

讲到这里，大概有读者会想争论机器的可能性，毕竟只要够复杂、够先进，机器或许就能「自主地」完成任务，弥补过失。就拿自动驾驶这个例子来说，前些年最大的挑战无非是现实中复杂的路况让机器无法迅速适应，但这样的课题随着技术进步都被逐一解决。总有一天，计算机不仅能处理这些既定的任务，还能通过分析海量的数据，帮助人类做出决策……如此云云，笔者将在后文给出回应。

***

# 二、学习与理解
如今的人工智能之所以厉害，离不开一个名为「机器学习」的领域突飞猛进的发展。本文无意探讨任何具体的技术手段，总之现在的计算机程序，可以在不给出强制命令的情况下，找到最好的解决问题的手段。让我们回到开头的AlphaGo Zero的例子，它不需要像任何围棋选手或爱好者一样学习棋谱，只需要教会它最基础的走子吃子与胜负判定的规则，它跟自己玩了三天三夜，就参悟了棋道，超越所有人类以及前辈AlphaGo，成为了世界最强的围棋选手。

它从（制）作者那里得到的东西只有一个，那就是「学习」本身的能力。虽然它天资愚钝，但凭借计算机超人的运算量与记忆力，以及不吃饭不睡觉的铁人意志，它掌握了一套技术，一套应变下棋然后获得胜利的技术，亦即一个程序。因此，它跟前文提到的简单的程序不一样，是一个借由名为「学习」的程序「自主地」孕育了「下棋」的程序。「机器学习」之所以在学界与业界大行其道，风靡一时，正是因为这套名为「学习」的程序可以运用到许多截然不同的领域。不仅仅是游戏、科技、生活，甚至是人类引以为豪的——艺术。

![下一个伦勃朗](http://upload-images.jianshu.io/upload_images/3052093-926b69723fc8999a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图即为用人工智能学习伦勃朗的画作后，由计算机新绘的仿作[^3]。这般美轮美奂，是让人觉得欣喜若狂，还是背脊发凉？

[^3]: 图片引自网络，参见「下一个伦勃朗（[The Next Rembrandt](https://www.nextrembrandt.com)）」。


然而笔者要说——
So what?
那又怎么样？

笔者无意质疑「机器学习」的技术，但就像所有会问笔者「你做的研究有什么用」的人一样，笔者也一样问：机器学习有什么用？

如果答案是解决问题，笔者毫无异议。譬如笔者自高考结束后考了驾照，至今没有一次上路过，如果能有全自动驾驶，真是再好不过。实际上许多支持自动驾驶的研究都表明，由机器驾驶会比由人力驾驶安全许多，我们应该大力推进自动驾驶。笔者看到的针对反对声音最有意思的辩驳是当年升降电梯从人工改成自动操作时，社会上也是许多反对，觉得没有操作员在电梯里怎么可能安全，实际上全新的自动电梯安全许多，也做好了充分的应对万一的准备。

但譬如笔者的研究的目的是增加人类对于神经元传递信号过程的理解，方法论是理性主义为主，辅之以经验主义。换个简单的说法，用数学工具推演自己的理论，最后把结果同实验数据比较，以证明理论是有道理的。笔者的目的绝不是做一个仿真的神经元出来，也不是为了让计算的结果完全与现实相符——虽然这些目的可能顺便达成，但如果只是为了达成目的，大可使用现成的软件。同理，许多难攻不破的数学猜想被多少人试验了几千万次，找不到一个反例，但为什么不直接认为它们就是正确的，仍要费尽心思去作证明呢？因为重点不（仅仅）是得到一个答案，而是增进人类对于自然的理解[^4]。

[^4]: 当然对于不同的人来说，对待不同问题的态度可能截然相反，有些人费劲一生去理解的东西，对于另一些可能只要一个结果就行。举一个不贴切的例子：一个尝遍全世界米其林三星的吃货，他只需要有钱消费就行了，他不需要会做菜。

再回到下棋，事实已经证明机器可以赢得比赛，远比世界冠军的人类要来得厉害。这是没错，可这对我们又有什么益处呢？AlphaGo Zero的作者宣称它能自己发现知识，那笔者作为一个围棋初学者，是不是可以拜它为师，从此登上人类围棋的顶峰呢？实际上机器下出来的棋谱有许多让人无法理解的地方，想要学习围棋提高水平，我们还只能跟着先「人」的脚步[^5]。机器是否通过图灵测试，是否具有「意志」这类形而上学问题姑且不论，如果它的知识无法让我们理解，我们又怎么能知道？我们又怎能放心？

[^5]: 当年AlphaGo的棋谱还是得靠围棋选手来解说。

另外，下棋的目的真的是为了赢得比赛吗？近年有新闻传出，日本一名将棋选手因为被怀疑比赛过程中以上厕所之名偷用手机借鉴计算机程序，被禁赛半载。如果所有选手都允许使用机器姑且不论，只是部分人偷用，就好像运动员偷用禁药一样，是极其不公平的。虽然这反而在客观上承认了机器的有用，但「完成既定任务」意义上的有用与「在不违反法律规定风序良俗」前提下的有用是截然不同的。换句话说，用机器程序代替律法程序是存在道德风险的，因为对于人与人之间的比赛来说「公平第一，比赛第二」绝非空谈。

那么是否公平地允许所有人使用机器就可以了呢？或许是的。这世上确有国际象棋世界比赛，允许人类与机器的组合。而且使用计算机的建议与禁药不同，不会危害选手的身体健康。然而笔者要问，人类一早发明了各种交通工具，普通人骑个摩托车，也一定比田径世界冠军跑得快，我们为什么还热衷于运动与竞技？只要借助机器的力量，我们可以比希腊众神「更快，更高，更强」，那我们为什么还要宣扬奥林匹克精神呢？

在试图回答这个问题之前，笔者略微整理一下本文目前为止的观点。虽然机器程序在完成特定任务上已经显示出超越人类的水平，但机器似乎不考虑伦理道德，也没有精神意志可言。——说到这里可能有人沉不住气要骂笔者了，第一节结尾留下的问题虽然得到了看似正面的应答，却把任务的复杂度偷换成了诸如道德或精神这些暧昧不清的问题，只要这些问题可以被定义清楚，那机器程序代替律法程序岂不是指日可待？

虽然我们每个人都或多或少地「理解」这些暧昧的概念，但到底该怎么清晰定义呢？在这里继续用围棋举个不恰当的例子：我们可以用棋子与棋盘下围棋，也可以下五子棋，甚至如果不嫌麻烦，可以下黑白棋；我们可以教会我们自己或机器这些使用同样器具的规则不同的游戏；很快，我们都在每一个游戏上输给机器——但不管输赢，我们会偏爱某个，擅长某个，讨厌某个，甚至因为输得太惨再也不愿意玩某个。我们当然地理解自己在游戏中得到的喜怒哀乐，那么那台无论哪个游戏都很擅长的机器呢？

虽然笔者原本想在这里就结束文章并且用一句「我早告诉过你们律法调整的对象是暧昧的」来引发读者的思考，但为了避免被人指摘是为了装逼才故意说得云里雾里，就容笔者在后文里提出更切实具体的疑问吧。

***

# 三、程序与责任
我们再来重复一下机器程序的难题：任务目标不明确怎么办？道德法律风险如何折算？人类情绪该作何考量？……在这里笔者再次明确地指出，这些难题并非源于机器，而是源于我们自身。机器程序难以解决这些问题，正是因为我们人类对于诸多概念暧昧不清。那么，用以调整人类行为的律法程序可以解决这些问题吗？

*__不可以。__*

律法程序，或是具有法律效益的法律、证明、合同，或是具有其他影响的道德、风俗、约定，绝没有一个完美不出问题的。哪怕是再简单的一个任务，譬如父母每天早上叫醒小孩去上学，既是由人来做，就会出现各种各样的问题。因为人是自主的，父母可以选择不做。但也因为人是自主，父母即使双双出差，也可以打电话，或是请别人来做。

这些非机器的程序不仅仅在面对问题的时候善于变通（变通对于机器程序来说不是困难的），更因为这些程序的主体是人，其本身便具有了无限维度的改善可能。譬如做一个番茄炒蛋，照着菜谱做不会太糟，但可能让一台机器来做也无非如此。但喜欢甜的人可能会放糖，没错，要机器多放点糖也没什么困难。但也许今天甜食吃腻了想吃得淡一点，也许有客人来所以摆盘的时候要好看一点。又突然在锅碗瓢盆叮叮当当的过程中哼出了段新旋律，作了一支新曲，风行一时……而机器为了完美地执行既定的美味任务，是不会在意好看甚至好听的——那是不是只要提前给这机器加上优化摆盘与噪音的任务，它就可以超越人类了呢？谁去加一个试试，笔者保证没有人想投资「会唱歌的全自动番茄炒蛋机」。

更进一步，我们必须清楚地意识道，「*出现问题了怎么办？*」只是一个小问题（技术问题），「*出现问题了谁负责？*」才是现实中最本质的问题（伦理问题）。在这里，让我们回到自动驾驶的例子。笔者在写此文的此时此刻，全世界虽然已经有不少国家或地区在法律上允许自动驾驶的汽车上路，但一旦发生车祸，相关责任依然按照现行的法律，由驾车人承担。前文已经提到，自动驾驶的技术日趋成熟，其事故率将远低于一般司机；也就是说，一个人聘请了一个世界顶级的老司机，一旦出事，却需要由这个雇主负责。如果要类比自动升降电梯的例子，那当然该有相关厂家全权负责维护修理，而厂家为了确保不是有特定个人在其中搞事情，势必得装上无死角的摄像头，最好还配上物业派人监管——真是这样的话，那老司机是没什么存在价值了。

写到这里，笔者想到了今年去世的前苏联军官斯坦尼斯拉夫·彼得罗夫。1983年9月26日凌晨，一个位于莫斯科核攻击早期预警基地的仪器显示多枚导弹正从美国袭来，当晚值班的彼得罗夫违反规定，独断这项预警乃是一场误会，决定不向上级汇报。在当时的冷战环境下，苏联中央一旦接到警报，几乎一定会还击……那估计我们中大多数人都不在这里了。试问，如果当时那个预警仪器直接接上了苏联核武器发射装置会怎么样？

彼得罗夫在接受采访时回忆道，「*电脑的读数很齐全（显示正有导弹袭来）。如果我将其上报，没有人会质疑的。*」我们必须庆幸，苏联没有鲁莽到把事关人类存亡的重要决定交给一台机器来做。这可能是碍于当时的技术水平，也可能是某些人通过掌握核武器使用权这一事实满足了个人的权力欲望……理由并不重要，重要的是一旦程序中牵扯到人，程序中的人就必须为此负责。而律法程序的最大特点在于，程序中的人最需要负责的时候，恰恰是违反程序的时候。

另外，律法程序还有个特点就是涉及的人越多，个体的责任就越小。一方面，常言道「法不责众」是说违法的人多了也就不好追责了。另一方面，对于律法的订立，判决或执行来说也一样。有些例子太敏感就不提，拿企业来说，投资者越少越倾向于无限责任，投资者越多则越倾向于有限责任。机器程序则很难说具有这个特点，毕竟它在原则与宗旨上就尽量减少人类对于中间环节的干涉，而这理应成为该程序，亦即该程序的主导者需要承担更多责任的理由。

不考虑制度设计、权力结构等等复杂的问题，我们终归是回到了上面所提的那个最为本质的伦理问题——
>*__程序一旦出错，到底是谁的责任？__*

这个问题在理论上或许是简单的[^6]，刘邦攻占咸阳后与百姓约法三章：「*杀人者死、伤人及盗抵罪*。」可在现实中，到底怎样才能证明凶杀？到底用什么东西才能抵罪？有太多事情的真相，我们或许永远也不得而知。律法程序不完美，有空子可钻，但在它调整之下的人，与那些试图利用它来达到自己目的的人一样，也是一样不完美的。恰恰因为没有人可以完全掌控它，恰恰因为它不可能掌控所有人，它才是最有用，也经常容易出错，最可信，也时刻遭人怀疑的，人类个体欲求的无奈妥协与群体意志的智慧结晶。

[^6]: 事实上就理论而言，也并不简单。让我们考虑一下道德悖论（也叫伦理困境），诸如电车难题。在这里，我们无需了解具体悖论的细节，只需要知道大多数悖论无非由这三个要素构成：
一、甲是不道德的；
二、乙是不道德的；
三、身处悖论中的人必须做出或甲或乙的选择。

***

# 结语

1. 在机器程序面临的诸多难题中，技术问题是小事，伦理问题才是大问题。
2. 伦理问题之所以关键，乃是因为我们人类自己都没有搞清楚这些问题。
3. 我们之所以不能把伦理问题交给机器解决，乃是因为一旦出错，我们不知道该由谁来承担责任。

最后，笔者再想把启蒙运动搬出来，却又想不出什么新辞，便只好援引[旧文](http://www.jianshu.com/p/2def48672024)了：

>*启蒙运动推崇理性——不应只是在科学研究之中，更应是在人性的自我发现与整个社会文明之中。*
