---
title: AI's Failure in High-school Level Mathematical Reasoning
category: trial
date: 2025-10-31 14:38:37
tags:
  - mathematics
  - AI
  - teaching
  - ChatGPT
  - Gemini
  - Claude
---
I've started to do more teaching in the recent years, and it has become my habit to test my AI assistants before asking my students to solve a question, especially for the formal ones as I'd like 'somebody' else to verify my own working.

I use ChatGPT, Claude, and Gemini (ordered alphabetically) regularly. Apparently, there are many cases in which I can't use them, according to the university policy or for my concerns over data privacy.

Today, I found this interesting example, a question from the May 2020 Pearson Edexcel Level 3 GCE paper on Further Mathematics (paper reference: 8FM0/01)--ChatGPT was the only one that succeeded in its first attempt.


![The Maths Question](/images/maths-question.jpg)

You may want to attempt this question yourself before coming back and reading about Claude and Gemini' failure.

---

In all cases, I just pasted the above snapshot into a new chat with the AI, and the prompt is simply 'Please solve this step by step'. 

For the record, I'm using the unpaid versions: ChatGPT-5, Claude Sonnet 4.5, and Gemini 2.5 Flash. Gemini 2.5 Pro also succeeded, but I tested this later and I'm not sure if its global memory somehow played a critical role.

# Claude

![1](/images/claude-1.jpg)
![2](/images/claude-2.jpg)
![3](/images/claude-3.jpg)
