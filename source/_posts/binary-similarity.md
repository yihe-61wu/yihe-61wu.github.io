---
title: Thoughts on Similarity Measure for Binary Vectors
category: log
date: 2020-09-13 22:22:22
tags:
  - similarity
  - ANN
---

A similarity measure can be mathematically defined as a function that maps two vector spaces onto the real numbers between 0 and 1 (inclusive). There are various similarity measures. The only universal property is: the value of the measure is larger, if the two vectors are more similar. <!-- more -->Any similarity measure naturally pairs with a dissimilarity measure. The two measures are sometimes denoted by **S** and **D** with subscriptions referring to the specific function in use, e.g., Cosine, Jaccard. **D** is also a common notation for distances, which is closely related to dissimilarity. In many cases, they are treated as interchangeable; for example, a Euclidean distance can be considered as a useful dissimilarity measure, or can be literally named as a Euclidean dissimilarity. For binary vectors, the Hamming distance is perhaps the most widely used in computer science. However, there exist(-ed?) arguments over if counting the number of mismatches is the best similarity measure, and many alternatives are proposed, e.g., the Jaccard index.

To my knowledge and mainly to my concern, the main issue is whether or not to address the asymmetry between 0’s and 1’s. Not like a typical binary code in information theory, a 1 means the presence of some feature, and a 0 means the absence of some feature. Intuitively, a match of 1’s (a shared presence) is more important than a match of 0’s (a shared absence). The first paper I found reflecting this intuition is [_Asymmetric binary similarity measures_](https://link.springer.com/content/pdf/10.1007/BF00377169.pdf) by Dr. D. P. Faith; it points out that the Jaccard index is not completely satisfactory in addressing such asymmetry. Faith proposed his novel similarity measure for binary vectors, which rewards a shared presence for 1 point, punishes a mismatch for 1 point, and considers a shared absence neutral, worthy of 0 points. After normalising the overall similarity so that it varies between 0 and 1, the Faith similarity measure asserts:

> **_S_**_Faith = _(_#1-match _+_ #0-match _/_ _2) /_ #total.

I like the idea, but Google scholar tells me it is not a widely accepted idea, with 63 citations; several of them are review papers, and most of the rest are papers in the same discipline about ecology, biodiversity, etc. I appreciate the idea, because I am working with neural networks, and neural coding tends to be sparse, i.e., 1’s are rare. Why does nobody use this similarity measure in machine learning or computational neuroscience? There must be hundreds of researchers smarter than me. Don’t they want to use something seemingly better?

After two days digging into the literature, I haven’t got a good answer. My best guess is that nowadays we have much better classifiers. A complicated (dis-)similarity is like a transform of the distance, particularly the Hamming distance for binary vectors. As the modern classifiers are much more powerful than the 80’s, it is unnecessary to summarise data in a similarity measure. For example, one can easily train a perceptron to classify binary vectors. Moreover, I don’t know any approach is particularly similarity-dependent in unsupervised training. Therefore, I was attempted to use a past tense in the last sentence of the first paragraph; nowadays we may pay little attention to this question, ‘which similarity measure is the best’.

Nonetheless, I think a similarity measure has more to offer than a classifier. One can at least visualise a data set with some similarity measure, despite a difficult thing to do with high-dimensional data. Why do we like visualisation? We humans are somewhat a black box capable of unsupervised learning when looking at data points. If we see a small cluster of data points sitting between two big clusters, we may think the small cluster is a result of some kind of interference between the two big clusters. A classifier cannot make such a decision without predetermined categories. The decision might be totally wrong if we visualise data in a deceptive way, and naturally some automated cluster detector is desired. However, can such a detector take over all usefulness from a similarity measure? Isn’t it always better to use a detector that directly deals with high-dimensional data, rather than a similarity measure that reduces multiple-bit information to one real value? Probably yes, if there is no extra computational complexity. For people caring only about their data and any information that can be extracted from it, an automated cluster detector must be a better tool than a simple similarity measure; some additional computational expense is not problematic nowadays. For people studying unsupervised learning machines, however, a similarity measure can be used as a benchmark, when comparing multiple methods.

To machine learning researchers, a similarity measure might seem ridiculously simple. If the relations between high-dimensional binary vectors can reduce to a single real-valued similarity, the data set might be boring in the sense that its clusters are probably linearly separable. The interesting and challenging problems are those desperately longing for a kernel support vector machine, but proper kernels are never known beforehand. Thereby, all hail deep neural networks!

As a computational neuroscientist, I study artificial neural networks, too. I care about their final performance, measured by classification accuracy, etc., but I care also about their ‘anatomy’ and the corresponding functions. I have no wet lab skills so I cannot dissecting a real brain, but I can dissect an artificial neural network. What should I do when there is no known underlying truth when comparing two things? Specifically, two binary vectors? If my unsupervised learning model can ‘recall’ some pattern with errors, how many bits of errors can be tolerated? If I use an existing classifier or detector to make the decision for me, what threshold of the confidence levels or error rates should I set for the classifier/detector?

Coming from a mathematical background, these questions are constantly annoying me when I read non-mathematical, scientific papers. In one seminar room, people develop some models, capable of generating a neuronal dendritic tree. How convincing the theory is! How beautiful the figures are! How can one be sure a generated dendritic morphology is similar to a real one? In another room, people show their demos of simulations on swarming intelligence. It really looks like a flock of birds; they can even use it to control robots, how nice! Again, how can I tell if the movements of the dots on the screen are similar to flying birds? Are our raw eyes good enough to make such decisions (if at least the data can be visualised in a low-dimensional space)? Or do we need an ultimate measure of usefulness, e.g., how many collisions a group of robots can minimise when running together?

Another reason that, I think, makes a similarity measure for binary vectors still an interesting topic to discuss, is really about modelling per se. The first thing one has to do when builds a computational model is to quantify observables. It is often quite natural to think of continuous observables as real numbers, e.g., distance. Discrete observables that can be ranked can be considered as natural numbers, and could sometimes be treated as a real number, e.g., population. In some cases, discrete observables are labels, categories, or binary states, that are indifferent if one permutes the labels, e.g., student id number. In many real applications like medical diagnosis, biogeographical characterisation, however, I agree with Faith that there is an asymmetric between 1’s (presence) and 0’s (absence).

Moreover, the binary asymmetry hypothesis is independent of any potential correlations among features (bits), which should probably be left to classifiers/detectors to deal with. The hypothesis seems more applicable in artificial neural networks, because we don’t have any prior knowledge in the significance of individual neurons, but we know a firing event is relatively rare. I recommend anybody working with binary neurons to think through perhaps the Faith similarity measure, although I’m unsure if the particular scoring scheme (1 for a 1-match, 0 for a 0-match, and -1 for a mismatch) is optimal; I guess it is possible to set the scores based on the expected sparsity in a neural network. Although we don’t expect a similarity measure to be helpful in improving classifier/detector performance, we should still expect two similar vectors to belong to the same category/cluster after all.